{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">SIT 112 - Data Science Concepts</span>\n",
    "\n",
    "---\n",
    "Lecturer: Duc Thanh Nguyen | duc.nguyen@deakin.edu.au<br />\n",
    "\n",
    "School of Information Technology, <br />\n",
    "Deakin University, VIC 3215, Australia.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Practical Session : Twitter API</span>\n",
    "\n",
    "**The purpose of this session is to teach you:**\n",
    "\n",
    "1. How to use Twitter API\n",
    "2. Distances\n",
    "3. Term-by-Document matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">1. Twitter API</span>\n",
    "\n",
    "To work with Twitter API, we use a package called `TwitterAPI`. You can install it by executing the cell below if you don't have it on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -U -I TwitterAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to collect data from the Twitter API you need an Access token and secret. For now we have provided you with them, but to obtain yours you can go to https://apps.twitter.com/, click on 'Create New App', fill the form and then click on 'Create your Twitter Application'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TwitterAPI import TwitterAPI\n",
    "\n",
    "CONSUMER_KEY = \"HLnOFAqRgkrJaaqMoy41a0nxj\"\n",
    "CONSUMER_SECRET = \"tz4GKwFn9dsqYv6Mnl9mzctSqV5LeOVGhAuvr1KW0c2LpLuprZ\"\n",
    "OAUTH_TOKEN = \"763929928219766784-AjLCTTzhDWx87EJxHpfeC7IPvFnQB1t\"\n",
    "OAUTH_TOKEN_SECRET = \"MTJThLdK6SkVzqnHEGvja2yUhIDSCZ8XCfNzjDTUgiXAu\"\n",
    "\n",
    "# Authonticating with your application credentials\n",
    "api = TwitterAPI(CONSUMER_KEY,\n",
    "                 CONSUMER_SECRET,\n",
    "                 OAUTH_TOKEN,\n",
    "                 OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to API. For a complete reference on what the API offers look at the [Twitter API documentation](https://dev.twitter.com/overview/api). For example we can search for tweets that contain a specific keyword or collect tweets from the Twitter stream. Twitter responses are in JSON format which we can easily parse into Python dictionary object.\n",
    "\n",
    "### <span style=\"color:#0b486b\">1.1 Search Tweets</span>\n",
    "\n",
    "You can query Twitter with a keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = api.request('search/tweets', {'q':'pizza'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over the reponse to print the Twitter message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a must I have to eat my pizza with ranch üôÑ\n",
      "tem umas fotos de pizza na tl to quase vomitando socorro\n",
      "s/o to my dad though for staying up and helping me unpack my car\n",
      "\n",
      "plus, get me some pizza rolls &amp; bbq sauce\n",
      "RT @druggie: just imagine how great life would be if pizza made u skinny\n",
      "RT @EricSjursen: I just ordered a Create Your Own, Breadsticks with Marinara Sauce - single order, Marinara online from Pizza Hut! https://‚Ä¶\n",
      "RT @shaunpls: ‚îè‚îì  \n",
      "‚îÉ‚îÉ‚ï±‚ï≤ in this\n",
      "‚îÉ‚ï±‚ï±‚ï≤‚ï≤  house\n",
      "‚ï±‚ï±‚ï≠‚ïÆ‚ï≤‚ï≤  we \n",
      "‚ñî‚ñè‚îó‚îõ‚ñï‚ñî    worship\n",
      "‚ï±‚ñî‚ñî‚ñî‚ñî‚ñî‚ñî‚ñî‚ñî‚ñî‚ñî‚ï≤  \n",
      "         Pineapples on pizza\n",
      "‚ï±‚ï±‚îè‚î≥‚îì‚ï≠‚ïÆ‚îè‚î≥‚îì ‚ï≤‚ï≤ \n",
      "‚ñî‚ñè‚îó‚îª‚îõ‚Ä¶\n",
      "RT @ClassicallyJosh: I'd like to publicly thank @CashPeyto  @paigepinky for introducing me to putting salt on your pizza rolls\n",
      "Pizza bagels are life.\n",
      "RT @lnedito: ¬´Diosito, cu√≠damela un chingo, que llegue bien a casa¬ª\n",
      "\n",
      "‚Äî Yo cuando pido pizza a domicilio.\n",
      "netflix + novela mexicana da luana + pizza = sabado perfeito\n",
      "RT @o_refinnej: PINEAPPLE IS SO GOOD AND IT DOES BELONG ON PIZZA, DON'T EVEN @ ME\n",
      "RT @FoodPornsx: Deep Dish Pizza https://t.co/RIroCstae9\n",
      "Netflix and pizza pls maybe chill\n",
      "Hoy comi pizza\n",
      "S√≥ vou comer um peda√ßo de pizza primeiro\n"
     ]
    }
   ],
   "source": [
    "for r in resp:\n",
    "    print r['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "1. Select a keyword and crawl some tweets from Twitter containing that keyword and then print them.\n",
    "2. Crawl 100 tweets containing this keyword and print them. Maybe you want to check Twitter API documentation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other parameters that you can set to restrict the response. For example the language of the tweets, or geographical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_type: popular, recent, mixed\n",
    "# geocode: lat,long,radius\n",
    "\n",
    "# geo coordinations of the desired place\n",
    "my_lat = 51.5;\n",
    "my_long = 0.12;\n",
    "\n",
    "resp = api.request('search/tweets', {'q':'house', \n",
    "                                     'count':'100', \n",
    "                                     'lang':'en', \n",
    "                                     'result_type':'recent',\n",
    "                                     'geocode':'{},{},100mi'.format(my_lat, my_long)})\n",
    "for r in resp:\n",
    "    print r['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise2:**\n",
    "\n",
    "check out the API documentation and narrow down your search results for Exercise1 using parameters other that keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the tweet text, you can retrieve other metadata from the Twitter response. For example the user who sent the tweet, whether the tweet is in reply to another user or is a retweet, how many times it is retweeted and so on. Since the response is parsed into a dictionary, use `keys()` function to see the fields that are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['statuses'][0]['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['statuses'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise3:**\n",
    "\n",
    "print user, place, and geo locations of tweets you have collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#0b486b\">2. Distances</span>\n",
    "\n",
    "`Distance` is a numerical description of how far apart objects are. It is a concrete way of describing what it means for elements of some space to be close or far away from each other, for example the distance between two vectors in an 2-dimensional space.\n",
    "\n",
    "Now that you have know how to represent an n-dimensional vector in Python with NumPy arrays, we will write a function as a metric to measure the distance between two vectors. There are multiple ways to measure the distance between two vectors. We will discuss Euclidean distance and cosine distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:#0b486b\">2.1 Euclidean Distance</span>\n",
    "\n",
    "Euclidean distance comes from Geometry. If we assume $\\mathbf{x}_{1}=\\left[x_{11},x_{12},\\ldots,x_{1n}\\right]$ and $\\mathbf{x}_{2}=\\left[x_{21},x_{22},\\ldots,x_{2n}\\right]$, then the Euclidean distance between $\\mathbf{x}_{1}$ and $\\mathbf{x}_{2}$ is defined as:\n",
    "\n",
    "$$d\\left(\\mathbf{x}_{1},\\mathbf{x}_{2}\\right)=\\sqrt{\\left(x_{11}-x_{21}\\right)^{2}+\\left(x_{12}-x_{22}\\right)^{2}+\\ldots+\\left(x_{1n}-x_{2n}\\right)^{2}}\n",
    "$$\n",
    "\n",
    "We can use array operators for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([2, 5, 4, 6, 8])\n",
    "x2 = np.array([3, 5, 6, 8, 6])\n",
    "\n",
    "print x1 - x2\n",
    "print (x1 - x2) ** 2\n",
    "print np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance1(x1, x2):\n",
    "    d = x1 - x2\n",
    "    d = d ** 2\n",
    "    return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([-1, 2, 0, 5])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "\n",
    "print euclidean_distance1(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since two vectors passed to the function should be the same size, it is better to perform a sanity check before applying the subtraction. Otherwise it will raise an error. We can do this by using `if - elif` statement or as a better practice by using `try - except`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def euclidean_distance2(x1, x2):\n",
    "    if x1.shape[0] != x2.shape[0]:\n",
    "        sys.exit('x1 and x2 are not the same size')\n",
    "    else:\n",
    "        d = x1 - x2\n",
    "        d = d ** 2\n",
    "        return np.sqrt(d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "euclidean_distance2(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance3(x1, x2):\n",
    "    try:\n",
    "        d = x1 - x2\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this cell\n",
    "\n",
    "x1 = np.array([-1, 2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 2])\n",
    "a = euclidean_distance3(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance4(x1, x2):\n",
    "    try:\n",
    "        d = np.array(x1) - np.array(x2)\n",
    "        d = np.power(d, 2)\n",
    "        return np.sqrt(d.sum())\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        # you can return a default value\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">2.2 cosine similarity and distance</span>\n",
    "\n",
    "Cosine similarity is a measure of similarity between two vectors based on the angle between them. Cosine similarity is widely used in information retrieval and text mining as a measure of similarity between documents and is defined as:\n",
    "\n",
    "$$S_{c}\\left(\\mathbf{x}_{1},\\mathbf{x_{2}}\\right)=\\frac{\\mathbf{x}_{1}.\\mathbf{x_{2}}}{\\parallel\\mathbf{x}_{1}\\parallel^{2}+\\parallel\\mathbf{x}_{2}\\parallel^{2}-\\mathbf{x}_{1}.\\mathbf{x_{2}}}$$\n",
    "\n",
    "\n",
    "Cosine similarity is particularly used in positive space where the outcome is bounded in [0, 1]. The cosine distance is defined as the complement to cosine similarity in positive space that is $D_{c}\\left(x_{1},x_{2}\\right)=1-S_{c}\\left(x_1,x_2\\right)$ where $D_c$ is the cosine distance and $S_c$ is the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([1,2,3])\n",
    "x2 = np.array([3,4,6])\n",
    "\n",
    "x1 * x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance(x1, x2):\n",
    "    try:\n",
    "        num = (x1*x2).sum()\n",
    "        denom = (x1*x1).sum() + (x2*x2).sum() - (x1*x2).sum()\n",
    "        num += 0.0    # or use np.astype(float) to make sure of float division\n",
    "        return 1 - num/denom\n",
    "    except ValueError as e:\n",
    "        print \"Vectors passed to the function are not the same size\"\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([2, 0, 5, 9])\n",
    "x2 = np.array([4, 2, 1, 0])\n",
    "cosine_distance(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">2.3 Term-by-Document matrix</span>\n",
    "\n",
    "A term-by-document matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the document collection. Each row corresponds to a document and each column correspond to a term. Thus the value that appears in row $j$ and column $i$ represents the frequency of appearing term $i$ in document $j$.\n",
    "\n",
    "We will represent two datasets with term-by-document matrix:\n",
    "\n",
    "* a collection of 100 Twitter messages about Geelong\n",
    "* a collection of 6 news articles (5 about Apple and 1 about politics)\n",
    "\n",
    "The data is already collected and stores in text files. Thus you will need to:\n",
    "\n",
    "* read the text files\n",
    "    * using file object\n",
    "* perform pre-processing\n",
    "    * using string methods\n",
    "    * using re package\n",
    "* construct the term-by-document matrix\n",
    "    * using numpy arrays and operations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter dataset\n",
    "First read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# get current working directory\n",
    "cwd = os.getcwd()   \n",
    "\n",
    "# join the subdirectory of the data and data file name\n",
    "file_path = os.path.join(cwd, \"data\\\\prac06\\\\tweets.txt\")\n",
    "\n",
    "# read the contents of the file and store it in a list\n",
    "with open(file_path) as fp:\n",
    "    tweets = fp.readlines()    \n",
    "for tweet in tweets:\n",
    "    print tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly when dealing with data, we have to perform some sort of data pre-processing. Data collection is often loosely controlled, resulting in out of the range values, missing values, and etc. Thus quality of the data is first and formost before running an analysis. This step is specific to the nature of the data. For example for text data it may consist of cleaning, normalization, tokenization, and etc. \n",
    "\n",
    "In this case, our pre-processing consists of:\n",
    "\n",
    "* converting all the words into lower case to remove the effect of the letter case\n",
    "* replacing the URLs with a simple string such as 'url'. From the previous cell, you should be able to see that many of the tweets contain a URL. Since we are not using them now, we can remove them or replace them.\n",
    "* Removing the punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "def pre_process(doc):\n",
    "    \"\"\"\n",
    "    pre-processes a doc\n",
    "      * Converts the tweet into lower case,\n",
    "      * removes the URLs,\n",
    "      * removes the punctuations\n",
    "      * tokenizes the tweet\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    # gettign rid of non ascii codes\n",
    "    doc = doc.decode('ascii', 'ignore')\n",
    "    \n",
    "    # repalcing URLs\n",
    "    url_pattern = \"http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+\"\n",
    "    doc = re.sub(url_pattern, 'url', doc) \n",
    "\n",
    "    punctuation = r\"\\(|\\)|#|\\'|\\\"|-|:|\\\\|\\/|!|_|,|=|;|>|<|\\.\"\n",
    "    doc = re.sub(punctuation, ' ', doc)\n",
    "    \n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print r['text']\n",
    "pre_process(r['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "Use the function provided to pre-process one of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def termdoc(docs):\n",
    "    \"\"\"\n",
    "    returns the term-by-document matrix and the vocabulary of the passed corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()   \n",
    "    termdoc_sparse = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # pre-process the doc\n",
    "        doc_tokens = pre_process(doc)\n",
    "        # computes the frequencies for doc\n",
    "        doc_sparse = Counter(doc_tokens)    \n",
    "\n",
    "        termdoc_sparse.append(doc_sparse)\n",
    "\n",
    "        # update the vocab\n",
    "        vocab.update(doc_sparse.iterkeys())  \n",
    "\n",
    "    vocab = list(vocab)\n",
    "    vocab.sort()\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    n_vocab = len(vocab)\n",
    "    termdoc_dense = np.zeros((n_docs, n_vocab), dtype=int)\n",
    "\n",
    "    for j, doc_sparse in enumerate(termdoc_sparse):\n",
    "        for term, freq in doc_sparse.iteritems():\n",
    "            termdoc_dense[j, vocab.index(term)] = freq\n",
    "            \n",
    "    return termdoc_dense, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_termdoc, tweets_vocab = termdoc(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets look at one of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "print tweets[j]\n",
    "print tweets_termdoc[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_vocab.index('beyond')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_termdoc[j][127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So baiscally, now each tweet is represented by a vector of size `len(tweets_vocab)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News dataset\n",
    "Similar to previous sections, the data is stored in text files names as'news1.txt', ..., 'news5.txt'. All we have to do is read the files, construct the corpus and send it to `termdoc()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_docs = 6\n",
    "cwd = os.getcwd()   \n",
    "news = []\n",
    "\n",
    "for j in xrange(1, n_docs+1):\n",
    "    filename = \"news{}.txt\".format(j)\n",
    "    file_path = os.path.join(cwd, \"data\\\\prac06\\\\{}\".format(filename))\n",
    "    with open(file_path) as fp:\n",
    "        news.append(fp.read())\n",
    "\n",
    "news_termdoc, news_vocab = termdoc(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now each news article is represented with a large vector of size `len(news_vocab)`. We can do many things with this representation. For example measuring the distance between two documents. The first 5 news articles are tech news and about Apple, but the 6th one is about politics. We expect that tech news be more similar to each other rather than to the politics news. In other words the distance between two articles from tech news should be less than the distance between a tech news article and a news article about politics. This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print cosine_distance(news_termdoc[1], news_termdoc[2])   # both aout Apple\n",
    "print cosine_distance(news_termdoc[1], news_termdoc[4])   # one from tech world, the other from politics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
